<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.13.0 by Michael Rose
  Copyright 2013-2018 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE.txt
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Autoencoder Tutorial - Sarbashis Das, Ph.D.</title>
<meta name="description" content="Topic: AutoEncoderAn Autoencoder neural network is a type of unsupervised learning algorithm that applies backpropagation and setting the target values to be equal to the inputs that means; y(i) = x(i)">



<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Sarbashis Das, Ph.D.">
<meta property="og:title" content="Autoencoder Tutorial">
<meta property="og:url" content="http://localhost:4000/installation/2019/AutoEncoder/">


  <meta property="og:description" content="Topic: AutoEncoderAn Autoencoder neural network is a type of unsupervised learning algorithm that applies backpropagation and setting the target values to be equal to the inputs that means; y(i) = x(i)">





  <meta name="twitter:site" content="@Sarbashis_Das">
  <meta name="twitter:title" content="Autoencoder Tutorial">
  <meta name="twitter:description" content="Topic: AutoEncoderAn Autoencoder neural network is a type of unsupervised learning algorithm that applies backpropagation and setting the target values to be equal to the inputs that means; y(i) = x(i)">
  <meta name="twitter:url" content="http://localhost:4000/installation/2019/AutoEncoder/">

  
    <meta name="twitter:card" content="summary">
    
  

  



  <meta property="article:published_time" content="2019-01-17T00:00:00+01:00">





  

  


<link rel="canonical" href="http://localhost:4000/installation/2019/AutoEncoder/">







  <script type="application/ld+json">
    {
      "@context": "http://schema.org",
      "@type": "Person",
      "name": "Sarbashis",
      "url": "http://localhost:4000",
      "sameAs": null
    }
  </script>







<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Sarbashis Das, Ph.D. Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<!--[if lte IE 9]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single">

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <a class="site-title" href="/">Sarbashis Das, Ph.D.</a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/Research/" >Research</a>
            </li><li class="masthead__menu-item">
              <a href="/Publications/" >Publications</a>
            </li><li class="masthead__menu-item">
              <a href="/CV/" >CV</a>
            </li><li class="masthead__menu-item">
              <a href="/year-archive/" >Blog Posts</a>
            </li><li class="masthead__menu-item">
              <a href="/Contact/" >Contact</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="http://schema.org/Person">

  
    <div class="author__avatar">
      

      
        <img src="/assets/images/sarbasis2.jpg" alt="Sarbashis Das" itemprop="image">
      
    </div>
  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Sarbashis Das</h3>
    
    
      <p class="author__bio" itemprop="description">
        Computational Biology| Machine Learning | Deep Learning | Bioinformatics| Data Analyst.
      </p>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="http://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name">Stockholm, Sweden</span>
        </li>
      

      
        
          
        
          
        
          
            <li><a href="https://twitter.com/"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
          
        
          
            <li><a href="https://github.com/sarbashis"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
          
        
          
            <li><a href="https://www.linkedin.com/in/sarbashis/"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
          
        
          
            <li><a href="https://scholar.google.com/citations?user=MDDoPX0AAAAJ&hl=en"><i class="fas fa-fw fa-link" aria-hidden="true"></i> Google Scholar</a></li>
          
        
      

      

      
        <li>
          <a href="mailto:dsarbashis@gmail.com">
            <meta itemprop="email" content="dsarbashis@gmail.com" />
            <i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i> Email
          </a>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="Autoencoder Tutorial">
    <meta itemprop="description" content="Topic: AutoEncoderAn Autoencoder neural network is a type of unsupervised learning algorithm that applies backpropagation and setting the target values to be equal to the inputs that means; y(i) = x(i)">
    <meta itemprop="datePublished" content="January 17, 2019">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Autoencoder Tutorial
</h1>
          
            <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  9 minute read
</p>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
        <h2 id="topic-autoencoder">Topic: AutoEncoder</h2>
<p>An Autoencoder <strong>neural network</strong> is a type of unsupervised learning algorithm that applies backpropagation and setting the target values to be equal to the inputs that means;
 y<sup>(i)</sup> = x<sup>(i)</sup></p>

<p>Ref:http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/
https://www.deeplearningbook.org/contents/autoencoders.html</p>

<p>The model is actually try to learn a function <em>h</em><sub>W,b</sub>(X) $\approx$ X which means that the fuction is trying to learn an approximatly identity function. The model is a neural network with 3 layers. The input layer size is the size of X and output layer also have same size while the hidden layer are of varing size. The number of neurons in the hidden layer may be less or more than the input layer. Lets assume the number of neuron in the hidden layer is 50 and the output layer is 100. So the function is force to learn compressed representation of the input data. If the input features were each independent of one another, this compression and subsequent reconstructioin would be very difficult task. However if there is some sort of structure exist for example correlation between the features then this stucture can be learned and leverage when forcing the input the network’s bottleneck.</p>

<p>There are many application of autoencoder such as:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- Dimension reduction
- Extract hidden pattern (information) in the data
- Recomendation system for the missing values
- Data denoising.
</code></pre></div></div>

<p>We are going to do a hands on exercise for better understanding.</p>

<p>There are different types of <strong>autoencoder</strong>:
     - Undercomplete autoencoder
     - Sparse autoencoder
     - Denoising autoencoder
     - Contractive autoencoder</p>

<p>### Undercomplete autoencoder
 ### Complete autoencoder</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Building an autoencoder</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># loading important libraries</span>

<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Input</span><span class="p">,</span> <span class="n">Dense</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">regularizers</span>
<span class="kn">from</span> <span class="nn">keras.callbacks</span> <span class="kn">import</span> <span class="n">TensorBoard</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Using TensorFlow backend.
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># encoding dimenison</span>
<span class="n">encoding_dim</span> <span class="o">=</span> <span class="mi">32</span>

<span class="n">input_img</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">784</span><span class="p">,))</span>

<span class="c"># encoded is the encoding representation of the input</span>
<span class="n">encoded</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">encoding_dim</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s">"relu"</span><span class="p">)(</span><span class="n">input_img</span><span class="p">)</span>

<span class="c"># decoded is the decoding representation of the encoded image</span>
<span class="n">decoded</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">"sigmoid"</span><span class="p">)(</span><span class="n">encoded</span><span class="p">)</span>

<span class="c"># This model map an input to its reconstruction</span>
<span class="n">autoencoder</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">input_img</span><span class="p">,</span><span class="n">decoded</span><span class="p">)</span>

<span class="c"># only encoded repsentation</span>
<span class="n">encoded</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">input_img</span><span class="p">,</span><span class="n">encoded</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Next is the decoded model</span>

<span class="n">encoded_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">encoding_dim</span><span class="p">,))</span>
<span class="n">decoder_layer</span> <span class="o">=</span> <span class="n">autoencoder</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">decoder</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">encoded_input</span><span class="p">,</span> <span class="n">decoder_layer</span><span class="p">(</span><span class="n">encoded_input</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Compile the model</span>
<span class="n">autoencoder</span><span class="o">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">"binary_crossentropy"</span><span class="p">,</span><span class="n">optimizer</span><span class="o">=</span><span class="s">"adadelta"</span><span class="p">)</span>
</code></pre></div></div>

<p>Let’s prepare our datasets</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">keras.datasets</span> <span class="kn">import</span> <span class="n">mnist</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">_</span><span class="p">),</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span><span class="n">_</span><span class="p">)</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(60000, 28, 28)
(10000, 28, 28)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Normalized the values between 0 to 1 </span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(60000, 28, 28)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># flatten the shape</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]))</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">x_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]))</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>784
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(60000, 784)
(10000, 784)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># training </span>
<span class="n">autoencoder</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">x_train</span><span class="p">,</span><span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">x_test</span><span class="p">),</span><span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">TensorBoard</span><span class="p">(</span><span class="n">log_dir</span><span class="o">=</span><span class="s">'autoencoderLog'</span><span class="p">)])</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Train on 60000 samples, validate on 10000 samples
Epoch 1/100
60000/60000 [==============================] - 2s 37us/step - loss: 0.3702 - val_loss: 0.2719
Epoch 2/100
60000/60000 [==============================] - 2s 33us/step - loss: 0.2648 - val_loss: 0.2543
Epoch 3/100
60000/60000 [==============================] - 2s 32us/step - loss: 0.2438 - val_loss: 0.2312
Epoch 4/100
60000/60000 [==============================] - 2s 32us/step - loss: 0.2230 - val_loss: 0.2130
Epoch 5/100
60000/60000 [==============================] - 2s 32us/step - loss: 0.2081 - val_loss: 0.2006
Epoch 6/100
60000/60000 [==============================] - 2s 32us/step - loss: 0.1973 - val_loss: 0.1912
Epoch 7/100
60000/60000 [==============================] - 2s 33us/step - loss: 0.1886 - val_loss: 0.1835
Epoch 8/100
60000/60000 [==============================] - 2s 33us/step - loss: 0.1816 - val_loss: 0.1771
Epoch 9/100
60000/60000 [==============================] - 2s 34us/step - loss: 0.1756 - val_loss: 0.1715
Epoch 10/100
60000/60000 [==============================] - 3s 46us/step - loss: 0.1704 - val_loss: 0.1667
Epoch 11/100
60000/60000 [==============================] - 2s 33us/step - loss: 0.1658 - val_loss: 0.1623
Epoch 12/100
60000/60000 [==============================] - 2s 35us/step - loss: 0.1616 - val_loss: 0.1582
Epoch 13/100
60000/60000 [==============================] - 2s 36us/step - loss: 0.1579 - val_loss: 0.1548
Epoch 14/100
60000/60000 [==============================] - 2s 32us/step - loss: 0.1544 - val_loss: 0.1514
Epoch 15/100
60000/60000 [==============================] - 2s 34us/step - loss: 0.1512 - val_loss: 0.1482
Epoch 16/100
60000/60000 [==============================] - 2s 32us/step - loss: 0.1482 - val_loss: 0.1453
Epoch 17/100
60000/60000 [==============================] - 2s 32us/step - loss: 0.1455 - val_loss: 0.1426
Epoch 18/100
60000/60000 [==============================] - 2s 32us/step - loss: 0.1429 - val_loss: 0.1401
Epoch 19/100
60000/60000 [==============================] - 2s 32us/step - loss: 0.1404 - val_loss: 0.1378
Epoch 20/100
60000/60000 [==============================] - 2s 34us/step - loss: 0.1381 - val_loss: 0.1355
Epoch 21/100
60000/60000 [==============================] - 2s 34us/step - loss: 0.1360 - val_loss: 0.1333
Epoch 22/100
60000/60000 [==============================] - 2s 32us/step - loss: 0.1339 - val_loss: 0.1313
Epoch 23/100
60000/60000 [==============================] - 2s 33us/step - loss: 0.1319 - val_loss: 0.1294
Epoch 24/100
60000/60000 [==============================] - 2s 32us/step - loss: 0.1300 - val_loss: 0.1275
Epoch 25/100
60000/60000 [==============================] - 2s 34us/step - loss: 0.1282 - val_loss: 0.1258
Epoch 26/100
60000/60000 [==============================] - 2s 34us/step - loss: 0.1265 - val_loss: 0.1241
Epoch 27/100
60000/60000 [==============================] - 3s 49us/step - loss: 0.1249 - val_loss: 0.1225
Epoch 28/100
60000/60000 [==============================] - 2s 35us/step - loss: 0.1234 - val_loss: 0.1211
Epoch 29/100
60000/60000 [==============================] - 2s 33us/step - loss: 0.1219 - val_loss: 0.1196
Epoch 30/100
60000/60000 [==============================] - 2s 34us/step - loss: 0.1206 - val_loss: 0.1183
Epoch 31/100
60000/60000 [==============================] - 2s 34us/step - loss: 0.1193 - val_loss: 0.1171
Epoch 32/100
60000/60000 [==============================] - 2s 33us/step - loss: 0.1182 - val_loss: 0.1160
Epoch 33/100
60000/60000 [==============================] - 2s 33us/step - loss: 0.1171 - val_loss: 0.1150
Epoch 34/100
60000/60000 [==============================] - 2s 31us/step - loss: 0.1161 - val_loss: 0.1141
Epoch 35/100
60000/60000 [==============================] - 2s 30us/step - loss: 0.1152 - val_loss: 0.1132
Epoch 36/100
60000/60000 [==============================] - 2s 31us/step - loss: 0.1144 - val_loss: 0.1123
Epoch 37/100
60000/60000 [==============================] - 2s 31us/step - loss: 0.1136 - val_loss: 0.1116
Epoch 38/100
60000/60000 [==============================] - 2s 31us/step - loss: 0.1129 - val_loss: 0.1109
Epoch 39/100
60000/60000 [==============================] - 2s 30us/step - loss: 0.1122 - val_loss: 0.1103
Epoch 40/100
60000/60000 [==============================] - 2s 30us/step - loss: 0.1116 - val_loss: 0.1097
Epoch 41/100
60000/60000 [==============================] - 2s 31us/step - loss: 0.1110 - val_loss: 0.1091
Epoch 42/100
60000/60000 [==============================] - 2s 31us/step - loss: 0.1105 - val_loss: 0.1086
Epoch 43/100
60000/60000 [==============================] - 3s 42us/step - loss: 0.1100 - val_loss: 0.1082
Epoch 44/100
60000/60000 [==============================] - 2s 33us/step - loss: 0.1096 - val_loss: 0.1077
Epoch 45/100
60000/60000 [==============================] - 2s 33us/step - loss: 0.1092 - val_loss: 0.1073
Epoch 46/100
60000/60000 [==============================] - 2s 41us/step - loss: 0.1088 - val_loss: 0.1069
Epoch 47/100
60000/60000 [==============================] - 3s 48us/step - loss: 0.1084 - val_loss: 0.1066
Epoch 48/100
60000/60000 [==============================] - 3s 57us/step - loss: 0.1080 - val_loss: 0.1062
Epoch 49/100
60000/60000 [==============================] - 3s 45us/step - loss: 0.1077 - val_loss: 0.1059
Epoch 50/100
60000/60000 [==============================] - 2s 41us/step - loss: 0.1074 - val_loss: 0.1056
Epoch 51/100
60000/60000 [==============================] - 2s 38us/step - loss: 0.1071 - val_loss: 0.1053
Epoch 52/100
60000/60000 [==============================] - 2s 41us/step - loss: 0.1068 - val_loss: 0.1050
Epoch 53/100
60000/60000 [==============================] - 2s 37us/step - loss: 0.1065 - val_loss: 0.1047
Epoch 54/100
60000/60000 [==============================] - 2s 36us/step - loss: 0.1063 - val_loss: 0.1045
Epoch 55/100
60000/60000 [==============================] - 2s 39us/step - loss: 0.1060 - val_loss: 0.1043
Epoch 56/100
60000/60000 [==============================] - 3s 46us/step - loss: 0.1058 - val_loss: 0.1041
Epoch 57/100
60000/60000 [==============================] - 3s 52us/step - loss: 0.1056 - val_loss: 0.1038
Epoch 58/100
60000/60000 [==============================] - 3s 48us/step - loss: 0.1054 - val_loss: 0.1036
Epoch 59/100
60000/60000 [==============================] - 3s 45us/step - loss: 0.1052 - val_loss: 0.1034
Epoch 60/100
60000/60000 [==============================] - 3s 53us/step - loss: 0.1050 - val_loss: 0.1032
Epoch 61/100
60000/60000 [==============================] - 2s 33us/step - loss: 0.1048 - val_loss: 0.1030
Epoch 62/100
60000/60000 [==============================] - 3s 42us/step - loss: 0.1046 - val_loss: 0.1028
Epoch 63/100
60000/60000 [==============================] - 2s 35us/step - loss: 0.1044 - val_loss: 0.1027
Epoch 64/100
60000/60000 [==============================] - 2s 41us/step - loss: 0.1042 - val_loss: 0.1025
Epoch 65/100
60000/60000 [==============================] - 2s 35us/step - loss: 0.1041 - val_loss: 0.1024
Epoch 66/100
60000/60000 [==============================] - 2s 38us/step - loss: 0.1039 - val_loss: 0.1022
Epoch 67/100
60000/60000 [==============================] - 2s 36us/step - loss: 0.1038 - val_loss: 0.1020
Epoch 68/100
60000/60000 [==============================] - 2s 35us/step - loss: 0.1036 - val_loss: 0.1019
Epoch 69/100
60000/60000 [==============================] - 2s 33us/step - loss: 0.1035 - val_loss: 0.1018
Epoch 70/100
60000/60000 [==============================] - 2s 34us/step - loss: 0.1033 - val_loss: 0.1017
Epoch 71/100
60000/60000 [==============================] - 2s 33us/step - loss: 0.1032 - val_loss: 0.1015
Epoch 72/100
60000/60000 [==============================] - 2s 33us/step - loss: 0.1031 - val_loss: 0.1014
Epoch 73/100
60000/60000 [==============================] - 2s 35us/step - loss: 0.1030 - val_loss: 0.1013
Epoch 74/100
60000/60000 [==============================] - 3s 55us/step - loss: 0.1028 - val_loss: 0.1012
Epoch 75/100
60000/60000 [==============================] - 2s 35us/step - loss: 0.1027 - val_loss: 0.1010
Epoch 76/100
60000/60000 [==============================] - 3s 45us/step - loss: 0.1026 - val_loss: 0.1009
Epoch 77/100
60000/60000 [==============================] - 2s 36us/step - loss: 0.1025 - val_loss: 0.1008
Epoch 78/100
60000/60000 [==============================] - 2s 38us/step - loss: 0.1023 - val_loss: 0.1007
Epoch 79/100
60000/60000 [==============================] - 2s 37us/step - loss: 0.1022 - val_loss: 0.1006
Epoch 80/100
60000/60000 [==============================] - 3s 49us/step - loss: 0.1021 - val_loss: 0.1004
Epoch 81/100
60000/60000 [==============================] - 2s 40us/step - loss: 0.1020 - val_loss: 0.1003
Epoch 82/100
60000/60000 [==============================] - 3s 52us/step - loss: 0.1019 - val_loss: 0.1002
Epoch 83/100
60000/60000 [==============================] - 3s 49us/step - loss: 0.1017 - val_loss: 0.1001
Epoch 84/100
60000/60000 [==============================] - 2s 32us/step - loss: 0.1016 - val_loss: 0.1000
Epoch 85/100
60000/60000 [==============================] - 3s 48us/step - loss: 0.1015 - val_loss: 0.0999
Epoch 86/100
60000/60000 [==============================] - 2s 35us/step - loss: 0.1014 - val_loss: 0.0998
Epoch 87/100
60000/60000 [==============================] - 2s 36us/step - loss: 0.1013 - val_loss: 0.0996
Epoch 88/100
60000/60000 [==============================] - 2s 41us/step - loss: 0.1012 - val_loss: 0.0995
Epoch 89/100
60000/60000 [==============================] - 2s 37us/step - loss: 0.1010 - val_loss: 0.0994
Epoch 90/100
60000/60000 [==============================] - 2s 33us/step - loss: 0.1009 - val_loss: 0.0993
Epoch 91/100
60000/60000 [==============================] - 2s 37us/step - loss: 0.1008 - val_loss: 0.0992
Epoch 92/100
60000/60000 [==============================] - 2s 39us/step - loss: 0.1007 - val_loss: 0.0991
Epoch 93/100
60000/60000 [==============================] - 2s 37us/step - loss: 0.1006 - val_loss: 0.0990
Epoch 94/100
60000/60000 [==============================] - 2s 33us/step - loss: 0.1005 - val_loss: 0.0989
Epoch 95/100
60000/60000 [==============================] - 2s 35us/step - loss: 0.1004 - val_loss: 0.0988
Epoch 96/100
60000/60000 [==============================] - 2s 35us/step - loss: 0.1003 - val_loss: 0.0986
Epoch 97/100
60000/60000 [==============================] - 2s 33us/step - loss: 0.1002 - val_loss: 0.0985
Epoch 98/100
60000/60000 [==============================] - 2s 32us/step - loss: 0.1000 - val_loss: 0.0984
Epoch 99/100
60000/60000 [==============================] - 2s 32us/step - loss: 0.0999 - val_loss: 0.0984
Epoch 100/100
60000/60000 [==============================] - 2s 33us/step - loss: 0.0998 - val_loss: 0.0982





&lt;keras.callbacks.History at 0x134fde208&gt;
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">autoencoder</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_3 (InputLayer)         (None, 784)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 32)                25120     
_________________________________________________________________
dense_4 (Dense)              (None, 784)               25872     
=================================================================
Total params: 50,992
Trainable params: 50,992
Non-trainable params: 0
_________________________________________________________________
None
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">encoded_imgs</span> <span class="o">=</span> <span class="n">encoded</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">decoded_imgs</span> <span class="o">=</span> <span class="n">decoder</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">encoded_imgs</span><span class="p">)</span>
</code></pre></div></div>

<p>Lets plot with matplotlib</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n</span> <span class="o">=</span><span class="mi">10</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="c"># original image</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">x_test</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gray</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">get_xaxis</span><span class="p">()</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">get_yaxis</span><span class="p">()</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
    
    <span class="c"># Reconstructed image</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">decoded_imgs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gray</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">get_xaxis</span><span class="p">()</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">get_yaxis</span><span class="p">()</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>


<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/images/output_20_0.png" alt="png" /></p>


        
      </section>

      <footer class="page__meta">
        
        
  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/tags/#keras" class="page__taxonomy-item" rel="tag">keras</a>
    
    </span>
  </p>




  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/categories/#installation" class="page__taxonomy-item" rel="tag">installation</a>
    
    </span>
  </p>


        
          <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2019-01-17T00:00:00+01:00">January 17, 2019</time></p>
        
      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?via=Sarbashis_Das&text=Autoencoder+Tutorial%20http%3A%2F%2Flocalhost%3A4000%2Finstallation%2F2019%2FAutoEncoder%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2Finstallation%2F2019%2FAutoEncoder%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://plus.google.com/share?url=http%3A%2F%2Flocalhost%3A4000%2Finstallation%2F2019%2FAutoEncoder%2F" class="btn btn--google-plus" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Google Plus"><i class="fab fa-fw fa-google-plus" aria-hidden="true"></i><span> Google+</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2Finstallation%2F2019%2FAutoEncoder%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/installation/2018/How-to-setup-tensorflow-keras/" class="pagination--pager" title="How to setup tensorflow and keras in Ubuntu
">Previous</a>
    
    
      <a href="/installation/2019/Install-lightGBM-in-MAC-Mojave/" class="pagination--pager" title="Installing LightGBM on MacOS Mojave
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You may also enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/installation/2019/Setting-up-ML-envs-ubuntu/" rel="permalink">Setting up Machine Learning Environment in Ubuntu/Linux
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  2 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">This post is for setting up a Machine Learning environment for your Machine Projects in Linux/Ubuntu. Also important to keep in mind that you should keep tra...</p>
  </article>
</div>
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/installation/2019/How-to-configure-tensorboard-jupyter-inline/" rel="permalink">How to configure tensorboard jupyter inline
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  2 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">

</p>
  </article>
</div>
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/installation/2019/Install-lightGBM-in-MAC-Mojave/" rel="permalink">Installing LightGBM on MacOS Mojave
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  7 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Content

</p>
  </article>
</div>
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/installation/2018/How-to-setup-tensorflow-keras/" rel="permalink">How to setup tensorflow and keras in Ubuntu
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  less than 1 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Upgrade tensor flow by following command.

</p>
  </article>
</div>
        
      </div>
    </div>
  
  
</div>
    </div>

    

    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
      
        
      
        
          <li><a href="https://github.com/sarbashis"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
      
        
      
        
      
    

    <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2019 Sarbashis. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>
  <script src="https://use.fontawesome.com/releases/v5.3.1/js/all.js"></script>







    
<div class="page__comments">
<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/


var disqus_config = function () {
this.page.url = "https://sarbashis.github.io/installation/2019/AutoEncoder/" ;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = /installation/2019/AutoEncoder; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://sarbashis.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>


  


   <script id="dsq-count-scr" src="//sarbashis.disqus.com/count.js" async></script>
  </body>
</html>
